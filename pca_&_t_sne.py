# -*- coding: utf-8 -*-
"""PCA & t-SNE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cGVVIy84gyW4Qo8hjidYmdOAWkwBDxI8

#Dimensionality Reduction:- PCA & t-SNE on House Prices Prediction

About Dataset:

“79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa”. In turn, the task was to predict the sale price of houses based on these 79 explanatory variables.
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Data Cleaning"""

# Commented out IPython magic to ensure Python compatibility.
#Import Libraries 

import re                     #This module provides regular expression matching operations similar to those found in Perl.
import pandas as pd           #pandas used for providing fast, flexible, and expressive data structures designed.
from matplotlib import pyplot  #matplotlib.pyplot is a collection of command style functions that make matplotlib work like MATLAB. 
import numpy as np             #numpy used for mathematical calculations.
import matplotlib.pyplot as plt  #matplotlib.pyplot is a state-based interface to matplotlib. It provides a MATLAB-like way of plotting.
import seaborn as sns            #Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.

sns.set()                         #Set multiple theme parameters in one step.
# %matplotlib inline

#load the data from my drive 
df_train = pd.read_csv('/content/drive/MyDrive/Dimension_train.csv')

#delete columns with many missing data
df_train.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage'], axis = 1,inplace=True)

#Drop rows with missing data 
df_train.dropna(inplace=True)

from sklearn.model_selection import train_test_split
df_train = pd.get_dummies(df_train) #Getting dummies for categorical values

#Splitting test and train
X_train, X_test, y_train, y_test = train_test_split(df_train.loc[:, df_train.columns != 'SalePrice'], df_train['SalePrice'], test_size=0.25, random_state=42)

"""#Principal Component Analysis (PCA)

We use PCA when we want to reduce the number of variables (i.e. the number of dimensions) that encode our data.The algorithm creates one vector (or principal component) at a time, and each will be optimized for the maximum variance.

## PCA Decomposition

79 explanatory variables, or 200 after encoding dummies, aren’t necessarily too many for most popular regression algorithms. That said, I wanted to figure out if (a) we could speed the training of a regression algorithm by decomposing the original data with PCA and (b) if the principal components themselves could tell us something about the variance, and what the most important variables seem to be.

pca.fit_transform(): Fit the model with data and apply the dimensionality reduction on data.

Documentataion: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
"""

#Here I decompose each row into 10 principal components
from sklearn.decomposition import PCA

def pca_dec(data, n):
  pca = PCA(n)
  X_dec = pca.fit_transform(data)
  return X_dec, pca

#Decomposing the train set:
pca_train_results, pca_train = pca_dec(X_train, 10)

#Decomposing the test set:
pca_test_results, pca_test = pca_dec(X_test, 10)

#Creating a table with the explained variance ratio
names_pcas = [f"PCA Component {i}" for i in range(1, 11, 1)]
scree = pd.DataFrame(list(zip(names_pcas, pca_train.explained_variance_ratio_)), columns=["Component", "Explained Variance Ratio"])

"""This relative importance of principal components is important because it allows us to investigate not only how important the first few principal components are but also what the makeup of each component is, and determine which of the original variables have the highest contribution to that eigenvector. Each of the subcomponents of the first PCA, for example, corresponds to one of the 200 original variables, and specifically to the weight and direction that variable has to the overall component. Thus, taking the first principal component first, we looked into each subcomponent and ranked them for their magnitude (from high to low):

pd.Datafreame(): Two-dimensional, size-mutable, potentially heterogeneous tabular data.

Documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html

df.sort_values(): Sort by the values along either axis.

Documentation: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html
"""

#Sorting the values of the first principal component by how large each one is
df = pd.DataFrame({'PCA':pca_train.components_[0], 'Variable Names':list(X_train.columns)})
df = df.sort_values('PCA', ascending=False)

#Sorting the absolute values of the first principal component by magnitude
df2 = pd.DataFrame(df)
df2['PCA']=df2['PCA'].apply(np.absolute)
df2 = df2.sort_values('PCA', ascending=False)
#print(df2['Variable Names'][0:11])

df.head()

"""Categorization and How Well do the First Two Principle Components Explain Housing Prices?

A second investigation I undertook with PCA was as follows: if we categorize the sale prices of houses into 5 categories, ranging from Tier 1 (<100,000 USD) to Tier 5 (≤ 500,000 USD), and plot each datum according to their first two principal components, how well does PCA separate the different pricing categories? we decided to categorize prices to make it easier to read the graph.
"""

#Creating Price Categories for TSNE and PCA Visualization!! We may want to use
#this in training classification models as well
def what_bracket(sale_price):
  if sale_price <=100000:
    return "Tier 1"
  elif sale_price <=200000:
    return "Tier 2"
  elif sale_price <=300000:
    return "Tier 3"
  elif sale_price <= 500000:
    return "Tier 4"
  else:
    return "Tier 5"

y_tiers = y_train.apply(what_bracket)

#Here I decompose each image into 50 principal components
from sklearn.decomposition import PCA

def pca_dec(data, n):
  pca = PCA(n)
  X_dec = pca.fit_transform(data)
  return X_dec, pca

#Decomposing the train set:
pca_train_results, pca_train = pca_dec(X_train, 10)

#Decomposing the test set:
pca_test_results, pca_test = pca_dec(X_test, 10)

#Creating a table with the explained variance ratio
names_pcas = [f"PCA Component {i}" for i in range(1, 11, 1)]
scree = pd.DataFrame(list(zip(names_pcas, pca_train.explained_variance_ratio_)), columns=["Component", "Explained Variance Ratio"])
print(scree)

"""
The table above shows us exactly how "important" each principle component is in explaining the data. Clearly, the first component is considerably more important than the remaining 9. We can visualize how steep the difference is in the graph below:"""

#Creating a scree plot:
xs = np.linspace(0, 9, 10)

plt.scatter(xs, pca_train.explained_variance_ratio_)
plt.title("Scree Plot")
plt.xlabel("PCA Component")
plt.xticks(ticks=xs)
plt.ylabel("Explained Variance Ratio")

"""Knowing this can help us determine how useful PCA is in this problem, describing how the data is spread into different levels of price. Note that originally we had around 200 variables, and only with some dimensionality reduction technique like PCA can we actually visualize this data in two dimensions.

plt.figure(): Create a new figure, or activate an existing figure.

Documentation: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html

sns.Scatterplot(): Draw a scatter plot with possibility of several semantic groupings.

Documentation: https://seaborn.pydata.org/generated/seaborn.scatterplot.html
"""

#Plotting the first two PCA components, as well as the price tier of the house
#to see if PCA helps us distinguish price-points

import seaborn as sns
import matplotlib.pyplot as plt

first_comps = pca_train_results[:,0] #Taking the first PCA component for each
                                    #decomposed house's data

second_comps = pca_train_results[:,1]

plt.figure(figsize=(16,10))
sns.scatterplot(
    x=first_comps, y=second_comps,
    hue=y_tiers,
    palette=sns.color_palette("hls", 5),
    legend="full",
    alpha=0.3
)

plt.title("Houses Explained by their First Two PCA Elements")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

"""#Decomposing With TSNE

While PCA is great for linearly separable data, TSNE promises to be a better decomposition technique that maps each point onto only two or three values in terms of separating non-linearly separable data. TSNE is a complex algorithm that outputs two values based on a pairwise similarity between inputs and pairwise similarity of low-dimensional points in the embedding. It matches two distributions of these two similarities to be as close as possible (Derksen, 2016). In other words, it compares each pair of data and uses a normal kernel to determine how to weight their similarity based on the distance between the two points across their different variables. The algorithm then tries to cluster similar points as much as possible by pushing similar points together and different points further away.

Ultimately, with PCA we get two values, one for each distribution, that identify each datum. Given that our PCA analysis shows data that do not look clearly linearly separable, I went ahead and ran TSNE as well.
"""

from sklearn.manifold import TSNE

#Decomposing the data with TSNE, onto 2 dimensions
tsne_train = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500)
tsne_train_results = tsne_train.fit_transform(X_train)

tsne_test = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500)
tsne_test_results = tsne_test.fit_transform(X_test)

#Plotting the data as explained by the two TSNE components
first_tsne = tsne_train_results[:,0]
second_tsne = tsne_train_results[:,1]

import seaborn as sns
import matplotlib.pyplot as plt


plt.figure(figsize=(16,10))
sns.scatterplot(
    x=first_tsne, y=second_tsne,
    hue=y_tiers,
    palette=sns.color_palette("hls", 5),
    legend="full",
    alpha=0.3
)
plt.xlabel("Comp 1")
plt.ylabel("Comp 2")
plt.title("T-SNE Decomposed Data")

"""This graph makes it easier to identify each datum, but it doesn't necessarily help us find clusters according to price (color). If we could see clear clusters, that would be good news for prediction algorithms trying to determine how price changes."""